{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0tdZDnC8YqO"
   },
   "source": [
    "# Feature Engineering For Ashrae Energy Predcition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "d4cyjtPe8uSl",
    "outputId": "4d3e966d-7120-4fa8-ca59-2a4e90af8692"
   },
   "outputs": [],
   "source": [
    "!pip install meteocalc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import datetime\n",
    "from meteocalc import feels_like, Temp\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Wy7N8N6oDLtb",
    "outputId": "dab541ee-f6cd-45c9-eb41-edb4745fa248"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzUgLGQKE6zg"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.rows\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6wh_kjyGMve"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/gdrive/My Drive/train.csv')\n",
    "building_df = pd.read_csv('/content/gdrive/My Drive/building_metadata.csv')\n",
    "weather_df = pd.read_csv('/content/gdrive/My Drive/weather_train.csv')\n",
    "weather_test_df = pd.read_csv('/content/gdrive/My Drive/weather_test.csv')\n",
    "test_df = pd.read_csv('/content/gdrive/My Drive/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xIp5SEa_wEEk"
   },
   "source": [
    "#### When we are loading the datasets, I dropped floor_count due to the 75% of the data is missing. For the building dataset.\n",
    "\n",
    "#### tranform the primary_use column to a numeric column by using one of the encoding strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gvzc_9Wnjq7e"
   },
   "source": [
    "## missing value filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzZRUwq9GMsd"
   },
   "outputs": [],
   "source": [
    "\n",
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    # Add Day,Week & Month features This dataset consits of hourly weather information. \n",
    "    # So we are going to fill missing values based on below new date features.\n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    # adding day ,moth and week as features\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day \n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "     \n",
    "    # fill missing air temperature with mean temperature of day of the month. \n",
    "    # Each month comes in a season and temperature varies lots in a season. So filling with yearly mean value is not a good idea.\n",
    "    \n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "\n",
    "    #Data is missing for most of days and even many consecutive days. So, first, calculate mean cloud_coverage of day of the month \n",
    "    # then fill rest missing values with last valid observation.(fillna with the method='ffill')\n",
    "    \n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"]) # imputing with daily means per site id\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    #fillna with the method='ffill' option. 'ffill' stands for 'forward fill' and will propagate last valid observation forward\n",
    "    \n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)  \n",
    "\n",
    "    \n",
    "    # Data is missing for most of days and even many consecutive days. So, first, calculate mean sea_level of day of the month \n",
    "    # then fill rest missing values with last valid observation.(fillna with the method='ffill')\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8f3xHEP47qo"
   },
   "source": [
    "### meteorological_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByU2AALp33Rw"
   },
   "source": [
    "#### Adding relative humidity  value with the help of given dew temperature and ait temperature \n",
    "\n",
    "https://bmcnoldy.rsmas.miami.edu/Humidity.html\n",
    "\n",
    "#### RH: =100*(EXP((17.625*TD)/(243.04+TD))/EXP((17.625*T)/(243.04+T)))\n",
    "\n",
    "#### adding feel like meteological feature by importing 'meteocalc' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3r7H9eR2PGC"
   },
   "outputs": [],
   "source": [
    "  \n",
    "    def get_meteorological_features(data):\n",
    "        def calculate_rh(df):\n",
    "            df['relative_humidity'] = 100 * (np.exp((17.625 * df['dew_temperature']) / (243.04 + df['dew_temperature'])) / np.exp((17.625 * df['air_temperature'])/(243.04 + df['air_temperature'])))\n",
    "        def calculate_fl(df):\n",
    "            flike_final = []\n",
    "            flike = []\n",
    "            # calculate Feels Like temperature\n",
    "            for i in range(len(df)):\n",
    "                at = df['air_temperature'][i]\n",
    "                rh = df['relative_humidity'][i]\n",
    "                ws = df['wind_speed'][i]\n",
    "                flike.append(feels_like(Temp(at, unit = 'C'), rh, ws))\n",
    "            for i in range(len(flike)):\n",
    "                flike_final.append(flike[i].f)\n",
    "            df['feels_like'] = flike_final\n",
    "            del flike_final, flike, at, rh, ws\n",
    "        calculate_rh(data)\n",
    "        calculate_fl(data)\n",
    "        return data\n",
    "\n",
    "    weather_df = get_meteorological_features(weather_df)\n",
    "    \n",
    "    return weather_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BalzK2JK0V2-"
   },
   "outputs": [],
   "source": [
    "def features_engineering(df):\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df.sort_values(\"timestamp\")\n",
    "    df.reset_index(drop=True)\n",
    "    \n",
    "    # Add more features\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"dayofweek\"] = df[\"timestamp\"].dt.weekday\n",
    "\n",
    "    df['group'] = df['timestamp'].dt.month\n",
    "    df['group'].replace((1, 2, 3, 4), 1, inplace = True)\n",
    "    df['group'].replace((5, 6, 7, 8), 2, inplace = True)\n",
    "    df['group'].replace((9, 10, 11, 12), 3, inplace = True)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LeP5FeXiNB4U"
   },
   "outputs": [],
   "source": [
    "weather_df = fill_weather_dataset(weather_df)\n",
    "weather_test_df = fill_weather_dataset(weather_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XB2FBpEyUu7Y"
   },
   "outputs": [],
   "source": [
    "weather_df  = features_engineering(weather_df )\n",
    "weather__test_df  = features_engineering(weather_test_df )\n",
    "train_df  = features_engineering(train_df)\n",
    "test_df  = features_engineering(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlKVCWVV5xwA"
   },
   "source": [
    "### Adding Holiday features for each site id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55nzbL8ENPdB"
   },
   "outputs": [],
   "source": [
    "location=pd.DataFrame()\n",
    "location['site_id']=np.arange(0,16)\n",
    "\n",
    "##https://www.kaggle.com/datadugong/locate-better-cities-by-weather-temp-fill-nans\n",
    "##https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115698\n",
    "##https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature\n",
    "\n",
    "location['city']=['Orlando','Heathrow','Tempe','Washington','Berkeley','Southampton',\\\n",
    "                     'Washington','Ottowa','Orlando','Austin','Saltlake','Ottowa','Dublin',\\\n",
    "                      'Minneapolis','Philadelphia','Rochester']\n",
    "location['country']=['US','UK','US','US','US','UK',\\\n",
    "                    'US','Montreal','US','US','US','Montreal','Ireland',\\\n",
    "                    'US','US','US']\n",
    "weather_df= weather_df.merge(location, on='site_id', how='left')\n",
    "weather_test_df= weather_test_df.merge(location, on='site_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atEA4DweNcyR"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "## https://www.geeksforgeeks.org/python-holidays-library/\n",
    "## https://towardsdatascience.com/5-minute-guide-to-detecting-holidays-in-python-c270f8479387\n",
    "import holidays\n",
    "UK=[]\n",
    "for ptr in holidays.UnitedKingdom(years=2016).keys():\n",
    "    UK.append(str(ptr))\n",
    "for ptr in holidays.UnitedKingdom(years=2017).keys(): #2017 year holydays in uk\n",
    "    UK.append(str(ptr))\n",
    "for ptr in holidays.UnitedKingdom(years=2018).keys():\n",
    "    UK.append(str(ptr))\n",
    "    UK.append('2019-01-01')\n",
    "IR=[]\n",
    "for ptr in holidays.Ireland(years=2016).keys():  #2016 year holydays in ireland\n",
    "    IR.append(str(ptr))\n",
    "for ptr in holidays.Ireland(years=2017).keys():\n",
    "    IR.append(str(ptr))\n",
    "for ptr in holidays.Ireland(years=2018).keys():\n",
    "    IR.append(str(ptr))\n",
    "    IR.append('2019-01-01')\n",
    "US=[]\n",
    "for ptr in holidays.UnitedStates(years=2016).keys(): #2016 year holydays in US\n",
    "    US.append(str(ptr))\n",
    "for ptr in holidays.UnitedStates(years=2017).keys():\n",
    "    US.append(str(ptr))\n",
    "for ptr in holidays.UnitedStates(years=2018).keys():\n",
    "    US.append(str(ptr))\n",
    "    US.append('2019-01-01')\n",
    "CA=[]\n",
    "for ptr in holidays.Canada(years=2016).keys():   #2016 year holydays in Canada\n",
    "    CA.append(str(ptr))\n",
    "for ptr in holidays.Canada(years=2017).keys():\n",
    "    CA.append(str(ptr))\n",
    "for ptr in holidays.Canada(years=2018).keys():\n",
    "    CA.append(str(ptr))\n",
    "    CA.append('2019-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34tRN8MoM-6d"
   },
   "outputs": [],
   "source": [
    "def is_holiday(df):\n",
    "    df['is_holiday']=[0]*(df.shape[0])\n",
    "    df.loc[df['country']=='US','is_holiday']=(df['timestamp'].dt.date.astype('str').isin(US)).astype(int)\n",
    "    df.loc[df['country']=='UK','is_holiday']=(df['timestamp'].dt.date.astype('str').isin(UK)).astype(int)\n",
    "    df.loc[df['country']=='Montreal','is_holiday']=(df['timestamp'].dt.date.astype('str').isin(CA)).astype(int)\n",
    "    df.loc[df['country']=='Ireland','is_holiday']=(df['timestamp'].dt.date.astype('str').isin(IR)).astype(int)\n",
    "    return df\n",
    "# get the holiday\n",
    "weather_df=is_holiday(weather_df)\n",
    "weather_test_df=is_holiday(weather_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "colab_type": "code",
    "id": "zxtchq8-GMq-",
    "outputId": "024919d9-d5bc-4c2e-eb64-6fb355df3069",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_df = train_df.merge(building_df, left_on='building_id',right_on='building_id',how='left')\n",
    "train_df = train_df.merge(weather_df,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\n",
    "\n",
    "test_df = test_df.merge(building_df,left_on='building_id',right_on='building_id',how='left')\n",
    "del building_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "test_df = test_df.merge(weather_test_df,how='left',on=['timestamp','site_id'])\n",
    "del weather_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# transform target variable\n",
    "train_df['meter_reading'] = np.log1p(train_df[\"meter_reading\"])\n",
    "train_df['square_feet'] =  np.log1p(train_df['square_feet'])\n",
    "\n",
    "test_df['square_feet'] =  np.log1p(test_df['square_feet'])\n",
    "    \n",
    "    # Remove Unused Columns\n",
    "     # Encode Categorical Data\n",
    "le = LabelEncoder()\n",
    "train_df[\"primary_use\"] = le.fit_transform(train_df[\"primary_use\"])\n",
    "test_df[\"primary_use\"] = le.fit_transform(test_df[\"primary_use\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmIbQ9wl7tCG"
   },
   "source": [
    "## removing outliers and floor_count feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrELAA2p32KF"
   },
   "outputs": [],
   "source": [
    "train_df.drop('floor_count',axis=1,inplace=True)\n",
    "test_df.drop('floor_count',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiHngaJf743C"
   },
   "source": [
    "#### As we saw in Eda building_id 1099 is considered as outier \n",
    "\n",
    "#### and we are also removing rows with electricity =0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "VvWoLU1q4b1E",
    "outputId": "2e7adb7f-d6cc-4c95-fa4e-fde72981a6b1"
   },
   "outputs": [],
   "source": [
    "idx_to_drop = list(train_df[(train_df['meter'] == 0) & (train_df['meter_reading'] == 0)].index)\n",
    "train_df.drop(idx_to_drop,axis='rows',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Hp3OTbYKJskR",
    "outputId": "d028bba2-643e-4109-b392-18f77247c055"
   },
   "outputs": [],
   "source": [
    "idx_to_drop = list(train_df[(train_df['building_id'] == 1099)].index)\n",
    "train_df.drop(idx_to_drop,axis='rows',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmHlHRaVli26"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['timestamp','country','city','hour_y','dayofweek_y','group_y','hour_x'], axis = 1)\n",
    "test_df = test_df.drop(['timestamp','country','city','hour_y','dayofweek_y','group_y','hour_x'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RsO4AETC7g0"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('/content/gdrive/My Drive/test_preprocessed1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8bamlXzSxGr"
   },
   "source": [
    "## summery of Notebook \n",
    "\n",
    "1) Filling missed values:\n",
    "\n",
    "    *  As cloud_coverage, precip_depth_1_hr, sea_level_pressure and wind_direction have significantly high missing values need to fill it\n",
    "\n",
    "    * Add Day,Week & Month features This dataset consits of hourly     weather information. So we are going to fill missing values based on  below new date features.\n",
    "  \n",
    "    a) fill missing air temperature with mean temperature of day of the month. Each month comes in a season and temperature varies lots in a season. So filling with yearly mean value is not a good idea.\n",
    "\n",
    "    b) Data is missing for most of days and even many consecutive days. So, first, calculate mean cloud_coverage, precip_depth_1_hr, sea_level_pressure and wind_direction  of day of the month and then fill rest missing values with last valid observation.(fillna with the method='ffill' option. 'ffill' stands for 'forward fill' and will propagate last valid observation forward)\n",
    "\n",
    "2) Adding Features to data:\n",
    "\n",
    "   a)meteorological_features:\n",
    "    \n",
    "    * as dew_temperatur and air_temperature were given \n",
    "    \n",
    "    * Relative_humidity : =100(EXP((17.625 * dewtemperature)/(243.04+dew_temperature))/EXP((17.625*air_temperature)/(243.04+air_temperature)))\n",
    "    \n",
    "    * By using this relation  added relative humidity feature to dataset \n",
    "\n",
    "    * feel_like feature: \n",
    "    \n",
    "    * We calculate a 'feels like temperature by taking into account the expected air temperature, relative humidity and the strength of the wind \n",
    "    \n",
    "    * As we have all features to find feel like temperature i used meteocalc library (https://pypi.org/project/meteocalc/)\n",
    "    to get it done\n",
    "    \n",
    "\n",
    "    b) Holiday_features\n",
    "    \n",
    "    * we got assumed contries for each site_id while  digging each  variable in data\n",
    "\n",
    "    * https://www.kaggle.com/datadugong/locate-better-cities-by-weather-temp-fill-nans\n",
    "    \n",
    "    * https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115698\n",
    "    \n",
    "    * https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature\n",
    "    \n",
    "    * by using python in build holiday library (contains holydays for most of the contries) we get holidays for us,uk,canada        and irland .\n",
    "    \n",
    "    * using these added the is_holiday feature to the dataset (https://towardsdatascience.com/5-minute-guide-to-detecting-holidays-in-python-c270f8479387)\n",
    "\n",
    "    * Then encoded the catogorical features \n",
    "\n",
    "3) Removal of Outliers\n",
    "\n",
    "     * As we found in eda building 1099 is an outlier we removed it\n",
    "\n",
    "     * There is no reason keeping zero electrical readings as it cant be zero .so those rows are also excluded from dataset \n",
    "     \n",
    "     * floor_coont feature is removed as it contains more than 76 percent of misssing values    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "feature_engineering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
